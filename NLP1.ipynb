{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0GMkjsJX9DFjHeYjxlS25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbvss199/colab_notebooks/blob/main/NLP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecvAMrbIMghn",
        "outputId": "6377cda9-901c-4aa0-9257-363292283129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure you have the necessary NLTK data files\n",
        "#punkt is a tokenization model provided by the Natural Language Toolkit (NLTK).\n",
        "#It's used for dividing a text into a list of sentences or words.\n",
        "nltk.download('punkt')\n",
        "\n",
        "#select the documents you want to read\n",
        "documents = ['document 1.txt', 'document 2.txt', 'document 3.txt']\n",
        "\n",
        "def read_and_tokenize(file_path,encoding='utf-8'):\n",
        "  with open(file_path, 'r', encoding=encoding) as file:\n",
        "    content = file.read()\n",
        "    tokens = word_tokenize(content)\n",
        "    return tokens\n",
        "\n",
        "tokenized_documents = {}\n",
        "# Read and tokenize each document\n",
        "for doc in documents:\n",
        "    file_path = f'/content/{doc}'  #\n",
        "    try:\n",
        "        tokenized_documents[doc] = read_and_tokenize(file_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "kLMBrzuzM-F2",
        "outputId": "1e1f14fe-6f86-4ef4-faba-58c1e1af6609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x92 in position 105: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a11566234c05>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/content/{doc}'\u001b[0m  \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtokenized_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a11566234c05>\u001b[0m in \u001b[0;36mread_and_tokenize\u001b[0;34m(file_path, encoding)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_and_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 105: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tokenization"
      ],
      "metadata": {
        "id": "tYUyfCVOTi11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure you have the necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "\n",
        "# List of document filenames\n",
        "documents = ['document 1.txt', 'document 2.txt', 'document 3.txt']\n",
        "\n",
        "# Function to read and tokenize the content of a file with a specified encoding\n",
        "def read_and_tokenize(file_path, encoding='utf-8'):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding=encoding) as file:\n",
        "            content = file.read()\n",
        "            tokens = word_tokenize(content)\n",
        "        return tokens\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Error decoding {file_path} with {encoding}. Trying 'latin1' encoding.\")\n",
        "        with open(file_path, 'r', encoding='latin1') as file:\n",
        "            content = file.read()\n",
        "            tokens = word_tokenize(content)\n",
        "        return tokens\n",
        "\n",
        "# Dictionary to store tokenized content\n",
        "tokenized_documents = {}\n",
        "\n",
        "# Read and tokenize each document\n",
        "for doc in documents:\n",
        "    file_path = f'/content/{doc}'  # Adjust the path as needed\n",
        "    try:\n",
        "        tokenized_documents[doc] = read_and_tokenize(file_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# Print tokenized content of each document\n",
        "for doc, tokens in tokenized_documents.items():\n",
        "    print(f\"Tokens for {doc}:\")\n",
        "    print(tokens)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0xcWJqmNt43",
        "outputId": "1301b46d-5c03-4a6b-df42-7278f87e80d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error decoding /content/document 2.txt with utf-8. Trying 'latin1' encoding.\n",
            "Error decoding /content/document 3.txt with utf-8. Trying 'latin1' encoding.\n",
            "Tokens for document 1.txt:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'The', 'objective', 'is', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'NLP', 'involves', 'enabling', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'produce', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'valuable', 'and', 'meaningful', '.', 'OpenAI', ',', 'known', 'for', 'developing', 'advanced', 'language', 'models', 'like', 'ChatGPT', ',', 'highlights', 'the', 'importance', 'of', 'NLP', 'in', 'creating', 'intelligent', 'systems', 'that', 'can', 'understand', ',', 'respond', 'to', ',', 'and', 'generate', 'text', ',', 'making', 'technology', 'more', 'user-friendly', 'and', 'accessible', '.']\n",
            "\n",
            "\n",
            "Tokens for document 2.txt:\n",
            "['The', 'essence', 'of', 'Natural', 'Language', 'Processing', 'lies', 'in', 'making', 'computers', 'understand', 'the', 'natural', 'language', '.', 'That\\x92s', 'not', 'an', 'easy', 'task', 'though', '.', 'Computers', 'can', 'understand', 'the', 'structured', 'form', 'of', 'data', 'like', 'spreadsheets', 'and', 'tables', 'in', 'the', 'database', ',', 'but', 'human', 'languages', ',', 'texts', ',', 'and', 'voices', 'form', 'an', 'unstructured', 'category', 'of', 'data', ',', 'and', 'it', 'becomes', 'difficult', 'for', 'the', 'computer', 'to', 'understand', 'it', ',', 'and', 'there', 'is', 'the', 'need', 'for', 'Natural', 'Language', 'Processing', '.', 'There\\x92s', 'a', 'lot', 'of', 'natural', 'language', 'data', 'out', 'there', 'in', 'various', 'forms', 'and', 'it', 'would', 'get', 'very', 'easy', 'if', 'computers', 'can', 'understand', 'and', 'process', 'that', 'data', '.', 'We', 'can', 'train', 'the', 'models', 'in', 'accordance', 'with', 'expected', 'output', 'in', 'different', 'ways', '.', 'Humans', 'have', 'been', 'writing', 'for', 'thousands', 'of', 'years', ',', 'there', 'are', 'a', 'lot', 'of', 'literature', 'pieces', 'available', ',', 'and', 'it', 'would', 'be', 'great', 'if', 'we', 'make', 'computers', 'understand', 'that', '.', 'But', 'the', 'task', 'is', 'never', 'going', 'to', 'be', 'easy', '.', 'Various', 'challenges', 'are', 'floating', 'out', 'there', 'like', 'understanding', 'the', 'correct', 'meaning', 'of', 'the', 'sentence', ',', 'correct', 'Named-Entity', 'Recognition', '(', 'NER', ')', ',', 'correct', 'prediction', 'of', 'various', 'parts', 'of', 'speech', ',', 'and', 'coreference', 'resolution', '(', 'the', 'most', 'challenging', 'thing', 'in', 'my', 'opinion', ')', '.', 'Computers', 'can\\x92t', 'truly', 'understand', 'the', 'human', 'language', '.', 'If', 'we', 'feed', 'enough', 'data', 'and', 'train', 'a', 'model', 'properly', ',', 'it', 'can', 'distinguish', 'and', 'try', 'categorizing', 'various', 'parts', 'of', 'speech', '(', 'noun', ',', 'verb', ',', 'adjective', ',', 'supporter', ',', 'etc', ')', 'based', 'on', 'previously', 'fed', 'data', 'and', 'experiences', '.', 'If', 'it', 'encounters', 'a', 'new', 'word', 'it', 'tried', 'making', 'the', 'nearest', 'guess', 'which', 'can', 'be', 'embarrassingly', 'wrong', 'few', 'times', '.', 'It\\x92s', 'very', 'difficult', 'for', 'a', 'computer', 'to', 'extract', 'the', 'exact', 'meaning', 'from', 'a', 'sentence', '.', 'For', 'example', '\\x96', 'The', 'boy', 'radiated', 'fire', 'like', 'vibes', '.', 'The', 'boy', 'had', 'a', 'very', 'motivating', 'personality', 'or', 'he', 'actually', 'radiated', 'fire', '?', 'As', 'you', 'see', 'over', 'here', ',', 'parsing', 'English', 'with', 'a', 'computer', 'is', 'going', 'to', 'be', 'complicated', '.', 'There', 'are', 'various', 'stages', 'involved', 'in', 'training', 'a', 'model', '.', 'Solving', 'a', 'complex', 'problem', 'in', 'Machine', 'Learning', 'means', 'building', 'a', 'pipeline', '.', 'In', 'simple', 'terms', ',', 'it', 'means', 'breaking', 'a', 'complex', 'problem', 'into', 'a', 'number', 'of', 'small', 'problems', ',', 'making', 'models', 'for', 'each', 'of', 'them', 'and', 'then', 'integrating', 'these', 'models', '.', 'A', 'similar', 'thing', 'is', 'done', 'in', 'NLP', '.', 'We', 'can', 'break', 'down', 'the', 'process', 'of', 'understanding', 'English', 'for', 'a', 'model', 'into', 'a', 'number', 'of', 'small', 'pieces', '.']\n",
            "\n",
            "\n",
            "Tokens for document 3.txt:\n",
            "['These', 'are', 'the', 'most', 'popular', 'applications', 'of', 'Natural', 'Language', 'Processing', 'and', 'chances', 'are', 'you', 'may', 'have', 'never', 'heard', 'of', 'them', '!', 'NLP', 'is', 'used', 'in', 'many', 'other', 'areas', 'such', 'as', 'social', 'media', 'monitoring', ',', 'translation', 'tools', ',', 'smart', 'home', 'devices', ',', 'survey', 'analytics', ',', 'etc', '.', 'Chances', 'are', 'you', 'may', 'have', 'used', 'Natural', 'Language', 'Processing', 'a', 'lot', 'of', 'times', 'till', 'now', 'but', 'never', 'realized', 'what', 'it', 'was', '.', 'But', 'now', 'you', 'know', 'the', 'insane', 'amount', 'of', 'applications', 'of', 'this', 'technology', 'and', 'how', 'it\\x92s', 'improving', 'our', 'daily', 'lives', '.', 'If', 'you', 'want', 'to', 'learn', 'more', 'about', 'this', 'technology', ',', 'there', 'are', 'various', 'online', 'courses', 'you', 'can', 'refer', 'to', '.', 'Are', 'you', 'passionate', 'about', 'data', 'and', 'looking', 'to', 'make', 'one', 'giant', 'leap', 'into', 'your', 'career', '?', 'Our', 'Data', 'Science', 'Course', 'will', 'help', 'you', 'change', 'your', 'game', 'and', ',', 'most', 'importantly', ',', 'allow', 'students', ',', 'professionals', ',', 'and', 'working', 'adults', 'to', 'tide', 'over', 'into', 'the', 'data', 'science', 'immersion', '.', 'Master', 'state-of-the-art', 'methodologies', ',', 'powerful', 'tools', ',', 'and', 'industry', 'best', 'practices', ',', 'hands-on', 'projects', ',', 'and', 'real-world', 'applications', '.', 'Become', 'the', 'executive', 'head', 'of', 'industries', 'related', 'to', 'Data', 'Analysis', ',', 'Machine', 'Learning', ',', 'and', 'Data', 'Visualization', 'with', 'these', 'growing', 'skills', '.', 'Ready', 'to', 'Transform', 'Your', 'Future', '?', 'Enroll', 'Now', 'to', 'Be', 'a', 'Data', 'Science', 'Expert', '!']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#stopwords,stemming"
      ],
      "metadata": {
        "id": "CJu-fazMTnII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "# Initialize stop words and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "for doc,tokens in tokenized_documents.items():\n",
        "  filtered_tokens=[word for word in tokens if word not in stop_words]\n",
        "  stemmed_tokens=[stemmer.stem(word) for word in filtered_tokens]\n",
        "  tokenized_documents[doc]=stemmed_tokens\n",
        "for doc, tokens in tokenized_documents.items():\n",
        "    print(f\"Tokens for {doc}:\")\n",
        "    print(tokens)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFbXkDLVRAkl",
        "outputId": "5f3a7a63-6bd2-4851-8e98-95a7f504dbe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens for document 1.txt:\n",
            "['natur', 'languag', 'process', '(', 'nlp', ')', 'branch', 'artifici', 'intellig', 'focu', 'interact', 'comput', 'human', 'natur', 'languag', '.', 'object', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data', '.', 'nlp', 'involv', 'enabl', 'machin', 'understand', ',', 'interpret', ',', 'produc', 'human', 'languag', 'way', 'valuabl', 'mean', '.', 'openai', ',', 'known', 'develop', 'advanc', 'languag', 'model', 'like', 'chatgpt', ',', 'highlight', 'import', 'nlp', 'creat', 'intellig', 'system', 'understand', ',', 'respond', ',', 'gener', 'text', ',', 'make', 'technolog', 'user-friendli', 'access', '.']\n",
            "\n",
            "\n",
            "Tokens for document 2.txt:\n",
            "['essenc', 'natur', 'languag', 'process', 'lie', 'make', 'comput', 'understand', 'natur', 'languag', '.', 'that\\x92', 'easi', 'task', 'though', '.', 'comput', 'understand', 'structur', 'form', 'data', 'like', 'spreadsheet', 'tabl', 'databa', ',', 'human', 'languag', ',', 'text', ',', 'voic', 'form', 'unstructur', 'categori', 'data', ',', 'becom', 'difficult', 'comput', 'understand', ',', 'need', 'natur', 'languag', 'process', '.', 'there\\x92', 'lot', 'natur', 'languag', 'data', 'variou', 'form', 'would', 'get', 'easi', 'comput', 'understand', 'process', 'data', '.', 'train', 'model', 'accord', 'expect', 'output', 'differ', 'way', '.', 'human', 'write', 'thousand', 'year', ',', 'lot', 'literatur', 'piec', 'avail', ',', 'would', 'great', 'make', 'comput', 'understand', '.', 'task', 'never', 'go', 'easi', '.', 'variou', 'challeng', 'float', 'like', 'understand', 'correct', 'mean', 'sentenc', ',', 'correct', 'named-', 'recognit', '(', 'ner', ')', ',', 'correct', 'predict', 'variou', 'part', 'speech', ',', 'coref', 'resolut', '(', 'challeng', 'thing', 'opinion', ')', '.', 'comput', 'can\\x92t', 'truli', 'understand', 'human', 'languag', '.', 'feed', 'enough', 'data', 'train', 'model', 'properli', ',', 'distinguish', 'tri', 'categor', 'variou', 'part', 'speech', '(', 'noun', ',', 'verb', ',', 'adject', ',', 'support', ',', 'etc', ')', 'base', 'previou', 'fed', 'data', 'experi', '.', 'encount', 'new', 'word', 'tri', 'make', 'nearest', 'guess', 'embarrassingli', 'wrong', 'time', '.', 'it\\x92', 'difficult', 'comput', 'extract', 'exact', 'mean', 'sentenc', '.', 'exampl', '\\x96', 'boy', 'radiat', 'fire', 'like', 'vibe', '.', 'boy', 'motiv', 'person', 'actual', 'radiat', 'fire', '?', 'see', ',', 'par', 'english', 'comput', 'go', 'complic', '.', 'variou', 'stage', 'involv', 'train', 'model', '.', 'solv', 'complex', 'problem', 'machin', 'learn', 'mean', 'build', 'pipelin', '.', 'simpl', 'term', ',', 'mean', 'break', 'complex', 'problem', 'number', 'small', 'problem', ',', 'make', 'model', 'integr', 'model', '.', 'similar', 'thing', 'done', 'nlp', '.', 'break', 'process', 'understand', 'english', 'model', 'number', 'small', 'piec', '.']\n",
            "\n",
            "\n",
            "Tokens for document 3.txt:\n",
            "['popular', 'applic', 'natur', 'languag', 'process', 'chanc', 'may', 'never', 'heard', '!', 'nlp', 'use', 'mani', 'area', 'social', 'media', 'monitor', ',', 'translat', 'tool', ',', 'smart', 'home', 'devic', ',', 'survey', 'analyt', ',', 'etc', '.', 'chanc', 'may', 'use', 'natur', 'languag', 'process', 'lot', 'time', 'till', 'never', 'realiz', '.', 'know', 'insan', 'amount', 'applic', 'technolog', 'it\\x92', 'improv', 'daili', 'live', '.', 'want', 'learn', 'technolog', ',', 'variou', 'onlin', 'cour', 'refer', '.', 'passion', 'data', 'look', 'make', 'one', 'giant', 'leap', 'career', '?', 'data', 'scienc', 'cour', 'help', 'chang', 'game', ',', 'importantli', ',', 'allow', 'student', ',', 'profess', ',', 'work', 'adult', 'tide', 'data', 'scienc', 'immer', '.', 'master', 'state-of-the-art', 'methodolog', ',', 'power', 'tool', ',', 'industri', 'best', 'practic', ',', 'hands-on', 'project', ',', 'real-world', 'applic', '.', 'becom', 'execut', 'head', 'industri', 'relat', 'data', 'analysi', ',', 'machin', 'learn', ',', 'data', 'visual', 'grow', 'skill', '.', 'readi', 'transform', 'futur', '?', 'enrol', 'data', 'scienc', 'expert', '!']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#td-idf score"
      ],
      "metadata": {
        "id": "HKM_riUnTxzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Create the TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Fit and transform the tokenized documents\n",
        "document_list=[' '.join(tokens) for tokens in tokenized_documents.values()]\n",
        "print(document_list)\n",
        "tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "\n",
        "# Convert TF-IDF matrix to a dense format and get feature names\n",
        "tfidf_dense = tfidf_matrix.toarray()\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_dense)\n",
        "\n",
        "# Print the feature names\n",
        "print(\"Feature Names:\")\n",
        "print(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue3jXRZqTIY4",
        "outputId": "522571f3-7cf1-4f8d-ae79-380c36c95de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur languag process ( nlp ) branch artifici intellig focu interact comput human natur languag . object program comput process analyz larg amount natur languag data . nlp involv enabl machin understand , interpret , produc human languag way valuabl mean . openai , known develop advanc languag model like chatgpt , highlight import nlp creat intellig system understand , respond , gener text , make technolog user-friendli access .', 'essenc natur languag process lie make comput understand natur languag . that\\x92 easi task though . comput understand structur form data like spreadsheet tabl databa , human languag , text , voic form unstructur categori data , becom difficult comput understand , need natur languag process . there\\x92 lot natur languag data variou form would get easi comput understand process data . train model accord expect output differ way . human write thousand year , lot literatur piec avail , would great make comput understand . task never go easi . variou challeng float like understand correct mean sentenc , correct named- recognit ( ner ) , correct predict variou part speech , coref resolut ( challeng thing opinion ) . comput can\\x92t truli understand human languag . feed enough data train model properli , distinguish tri categor variou part speech ( noun , verb , adject , support , etc ) base previou fed data experi . encount new word tri make nearest guess embarrassingli wrong time . it\\x92 difficult comput extract exact mean sentenc . exampl \\x96 boy radiat fire like vibe . boy motiv person actual radiat fire ? see , par english comput go complic . variou stage involv train model . solv complex problem machin learn mean build pipelin . simpl term , mean break complex problem number small problem , make model integr model . similar thing done nlp . break process understand english model number small piec .', 'popular applic natur languag process chanc may never heard ! nlp use mani area social media monitor , translat tool , smart home devic , survey analyt , etc . chanc may use natur languag process lot time till never realiz . know insan amount applic technolog it\\x92 improv daili live . want learn technolog , variou onlin cour refer . passion data look make one giant leap career ? data scienc cour help chang game , importantli , allow student , profess , work adult tide data scienc immer . master state-of-the-art methodolog , power tool , industri best practic , hands-on project , real-world applic . becom execut head industri relat data analysi , machin learn , data visual grow skill . readi transform futur ? enrol data scienc expert !']\n",
            "TF-IDF Matrix:\n",
            "[[0.13017859 0.         0.         0.         0.         0.13017859\n",
            "  0.         0.09900417 0.         0.         0.13017859 0.\n",
            "  0.         0.         0.13017859 0.         0.         0.\n",
            "  0.         0.         0.13017859 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.13017859 0.         0.         0.19800834 0.         0.\n",
            "  0.         0.13017859 0.         0.07688557 0.         0.13017859\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.13017859 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.13017859 0.         0.13017859 0.         0.\n",
            "  0.13017859 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.13017859\n",
            "  0.         0.19800834 0.         0.13017859 0.         0.\n",
            "  0.         0.         0.         0.26035717 0.13017859 0.13017859\n",
            "  0.09900417 0.         0.         0.13017859 0.38442785 0.13017859\n",
            "  0.         0.         0.         0.09900417 0.         0.\n",
            "  0.         0.         0.07688557 0.07688557 0.         0.\n",
            "  0.         0.09900417 0.         0.         0.09900417 0.\n",
            "  0.         0.         0.23065671 0.         0.         0.\n",
            "  0.         0.         0.23065671 0.         0.         0.13017859\n",
            "  0.         0.         0.         0.         0.13017859 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.15377114 0.13017859 0.         0.13017859 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.13017859 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.13017859 0.\n",
            "  0.         0.09900417 0.         0.09900417 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.19800834 0.         0.         0.13017859 0.13017859\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.09900417 0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.05177393 0.05177393 0.05177393 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.05177393 0.05177393 0.03937541\n",
            "  0.         0.10354786 0.         0.10354786 0.05177393 0.05177393\n",
            "  0.         0.05177393 0.05177393 0.10354786 0.         0.\n",
            "  0.         0.10354786 0.05177393 0.31500325 0.05177393 0.15532178\n",
            "  0.         0.         0.         0.1834711  0.05177393 0.\n",
            "  0.         0.05177393 0.10354786 0.05177393 0.05177393 0.15532178\n",
            "  0.05177393 0.         0.05177393 0.10354786 0.05177393 0.\n",
            "  0.05177393 0.03937541 0.05177393 0.05177393 0.         0.05177393\n",
            "  0.05177393 0.         0.05177393 0.05177393 0.05177393 0.10354786\n",
            "  0.05177393 0.         0.15532178 0.         0.         0.\n",
            "  0.         0.05177393 0.         0.10354786 0.05177393 0.\n",
            "  0.05177393 0.         0.         0.         0.         0.\n",
            "  0.         0.11812622 0.         0.         0.         0.\n",
            "  0.         0.         0.05177393 0.         0.         0.\n",
            "  0.03937541 0.03937541 0.         0.         0.1834711  0.\n",
            "  0.         0.03937541 0.05177393 0.11812622 0.05177393 0.\n",
            "  0.         0.07875081 0.03057852 0.12231406 0.         0.\n",
            "  0.         0.15750163 0.         0.         0.23625244 0.\n",
            "  0.05177393 0.05177393 0.12231406 0.05177393 0.05177393 0.05177393\n",
            "  0.03937541 0.05177393 0.03057852 0.05177393 0.10354786 0.\n",
            "  0.         0.         0.         0.         0.         0.05177393\n",
            "  0.05177393 0.05177393 0.10354786 0.         0.05177393 0.10354786\n",
            "  0.05177393 0.         0.         0.         0.05177393 0.05177393\n",
            "  0.15532178 0.12231406 0.         0.         0.         0.\n",
            "  0.05177393 0.10354786 0.         0.         0.         0.05177393\n",
            "  0.         0.         0.05177393 0.         0.         0.05177393\n",
            "  0.10354786 0.05177393 0.05177393 0.         0.10354786 0.\n",
            "  0.         0.05177393 0.10354786 0.05177393 0.05177393 0.\n",
            "  0.05177393 0.         0.05177393 0.         0.         0.05177393\n",
            "  0.10354786 0.         0.05177393 0.03937541 0.05177393 0.\n",
            "  0.05177393 0.10354786 0.05177393 0.05177393 0.         0.\n",
            "  0.03937541 0.         0.15532178 0.         0.         0.10354786\n",
            "  0.05177393 0.31500325 0.05177393 0.         0.         0.\n",
            "  0.19687703 0.05177393 0.05177393 0.         0.05177393 0.\n",
            "  0.03937541 0.05177393 0.         0.         0.10354786 0.05177393\n",
            "  0.05177393 0.05177393]\n",
            " [0.         0.         0.         0.         0.08550451 0.\n",
            "  0.08550451 0.06502839 0.08550451 0.08550451 0.         0.25651354\n",
            "  0.08550451 0.08550451 0.         0.         0.         0.06502839\n",
            "  0.08550451 0.         0.         0.         0.         0.\n",
            "  0.08550451 0.         0.         0.         0.17100903 0.08550451\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.17100903 0.         0.08550451 0.30300206 0.         0.\n",
            "  0.08550451 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.08550451\n",
            "  0.         0.06502839 0.         0.         0.08550451 0.\n",
            "  0.         0.08550451 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.08550451 0.08550451\n",
            "  0.         0.         0.08550451 0.         0.         0.08550451\n",
            "  0.         0.08550451 0.08550451 0.08550451 0.08550451 0.\n",
            "  0.08550451 0.         0.08550451 0.         0.08550451 0.08550451\n",
            "  0.17100903 0.08550451 0.         0.         0.         0.\n",
            "  0.         0.06502839 0.08550451 0.         0.10100069 0.\n",
            "  0.08550451 0.13005677 0.         0.         0.         0.08550451\n",
            "  0.08550451 0.06502839 0.05050034 0.05050034 0.08550451 0.08550451\n",
            "  0.17100903 0.         0.08550451 0.08550451 0.         0.08550451\n",
            "  0.         0.         0.10100069 0.         0.         0.\n",
            "  0.13005677 0.         0.05050034 0.         0.         0.\n",
            "  0.08550451 0.08550451 0.08550451 0.08550451 0.         0.\n",
            "  0.         0.         0.         0.08550451 0.         0.\n",
            "  0.         0.08550451 0.08550451 0.08550451 0.         0.\n",
            "  0.         0.10100069 0.         0.08550451 0.         0.08550451\n",
            "  0.         0.         0.08550451 0.08550451 0.08550451 0.\n",
            "  0.08550451 0.08550451 0.         0.         0.25651354 0.\n",
            "  0.         0.         0.         0.08550451 0.         0.08550451\n",
            "  0.08550451 0.         0.         0.         0.         0.08550451\n",
            "  0.         0.08550451 0.         0.08550451 0.         0.\n",
            "  0.         0.13005677 0.         0.         0.         0.08550451\n",
            "  0.         0.         0.         0.         0.08550451 0.08550451\n",
            "  0.06502839 0.17100903 0.         0.08550451 0.08550451 0.\n",
            "  0.         0.         0.         0.17100903 0.         0.\n",
            "  0.06502839 0.         0.         0.08550451 0.         0.08550451\n",
            "  0.         0.         0.08550451 0.08550451 0.         0.\n",
            "  0.         0.        ]]\n",
            "Feature Names:\n",
            "['access' 'accord' 'actual' 'adject' 'adult' 'advanc' 'allow' 'amount'\n",
            " 'analysi' 'analyt' 'analyz' 'applic' 'area' 'art' 'artifici' 'avail'\n",
            " 'base' 'becom' 'best' 'boy' 'branch' 'break' 'build' 'can' 'career'\n",
            " 'categor' 'categori' 'challeng' 'chanc' 'chang' 'chatgpt' 'complex'\n",
            " 'complic' 'comput' 'coref' 'correct' 'cour' 'creat' 'daili' 'data'\n",
            " 'databa' 'develop' 'devic' 'differ' 'difficult' 'distinguish' 'done'\n",
            " 'easi' 'embarrassingli' 'enabl' 'encount' 'english' 'enough' 'enrol'\n",
            " 'essenc' 'etc' 'exact' 'exampl' 'execut' 'expect' 'experi' 'expert'\n",
            " 'extract' 'fed' 'feed' 'fire' 'float' 'focu' 'form' 'friendli' 'futur'\n",
            " 'game' 'gener' 'get' 'giant' 'go' 'great' 'grow' 'guess' 'hands' 'head'\n",
            " 'heard' 'help' 'highlight' 'home' 'human' 'immer' 'import' 'importantli'\n",
            " 'improv' 'industri' 'insan' 'integr' 'intellig' 'interact' 'interpret'\n",
            " 'involv' 'it' 'know' 'known' 'languag' 'larg' 'leap' 'learn' 'lie' 'like'\n",
            " 'literatur' 'live' 'look' 'lot' 'machin' 'make' 'mani' 'master' 'may'\n",
            " 'mean' 'media' 'methodolog' 'model' 'monitor' 'motiv' 'named' 'natur'\n",
            " 'nearest' 'need' 'ner' 'never' 'new' 'nlp' 'noun' 'number' 'object' 'of'\n",
            " 'on' 'one' 'onlin' 'openai' 'opinion' 'output' 'par' 'part' 'passion'\n",
            " 'person' 'piec' 'pipelin' 'popular' 'power' 'practic' 'predict' 'previou'\n",
            " 'problem' 'process' 'produc' 'profess' 'program' 'project' 'properli'\n",
            " 'radiat' 'readi' 'real' 'realiz' 'recognit' 'refer' 'relat' 'resolut'\n",
            " 'respond' 'scienc' 'see' 'sentenc' 'similar' 'simpl' 'skill' 'small'\n",
            " 'smart' 'social' 'solv' 'speech' 'spreadsheet' 'stage' 'state' 'structur'\n",
            " 'student' 'support' 'survey' 'system' 'tabl' 'task' 'technolog' 'term'\n",
            " 'text' 'that' 'the' 'there' 'thing' 'though' 'thousand' 'tide' 'till'\n",
            " 'time' 'tool' 'train' 'transform' 'translat' 'tri' 'truli' 'understand'\n",
            " 'unstructur' 'use' 'user' 'valuabl' 'variou' 'verb' 'vibe' 'visual'\n",
            " 'voic' 'want' 'way' 'word' 'work' 'world' 'would' 'write' 'wrong' 'year']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cosine pairwise similarity for the documents\n"
      ],
      "metadata": {
        "id": "mnjhuGqgVPPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "cosine_sim\n",
        "# Print the cosine similarity matrix\n",
        "#After calculating the TF-IDF matrix, cosine_similarity(tfidf_matrix) computes the cosine similarity for all pairs of documents.\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(cosine_sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXvJ0FtKUPMc",
        "outputId": "e88526fb-4a30-4679-a66f-53280a6db9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "[[1.         0.36097678 0.13967942]\n",
            " [0.36097678 1.         0.14650358]\n",
            " [0.13967942 0.14650358 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Example documents\n",
        "documents = [\n",
        "    \"all that glitters is not gold\",\n",
        "    \"all is well that ends well\"\n",
        "]\n",
        "\n",
        "# Step 1: Tokenize the documents\n",
        "tokenized_documents = [doc.split() for doc in documents]\n",
        "\n",
        "# Step 2: Build the vocabulary\n",
        "all_words = [word for doc in tokenized_documents for word in doc]\n",
        "vocabulary = list(set(all_words))\n",
        "vocab_size = len(vocabulary)\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "# Step 3: Initialize the co-occurrence matrix\n",
        "co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=int)\n",
        "\n",
        "# Step 4: Populate the co-occurrence matrix\n",
        "window_size = 1\n",
        "\n",
        "for doc in tokenized_documents:\n",
        "    for i, word in enumerate(doc):\n",
        "        word_idx = word_to_index[word]\n",
        "        # Get the window range\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(doc), i + window_size + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:  # Don't count the word itself\n",
        "                context_word_idx = word_to_index[doc[j]]\n",
        "                co_occurrence_matrix[word_idx, context_word_idx] += 1\n",
        "                co_occurrence_matrix[context_word_idx, word_idx] += 1  # Since the matrix is symmetric\n",
        "\n",
        "# Step 5: Display the co-occurrence matrix\n",
        "df = pd.DataFrame(co_occurrence_matrix, index=vocabulary, columns=vocabulary)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "w-Nr3Nom8AQL",
        "outputId": "37194d94-91ca-4621-fc24-fa2b83ad684f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          well  not  all  ends  gold  is  glitters  that\n",
            "well         0    0    0     2     0   2         0     2\n",
            "not          0    0    0     0     2   2         0     0\n",
            "all          0    0    0     0     0   2         0     2\n",
            "ends         2    0    0     0     0   0         0     2\n",
            "gold         0    2    0     0     0   0         0     0\n",
            "is           2    2    2     0     0   0         2     0\n",
            "glitters     0    0    0     0     0   2         0     2\n",
            "that         2    0    2     2     0   0         2     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdOX9JsX8A6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}