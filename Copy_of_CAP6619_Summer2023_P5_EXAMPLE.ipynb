{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbvss199/colab_notebooks/blob/main/Copy_of_CAP6619_Summer2023_P5_EXAMPLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT9ZKo3q-2-F"
      },
      "source": [
        "# CAP 6619 - Deep Learning\n",
        "## Dr Marques\n",
        "## Project 5 - EXAMPLE\n",
        "## Image classification using the CIFAR-10 dataset and ChatGPT\n",
        "\n",
        "Total number of points: 100 (see guidelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'>Required info from You</font>**\n",
        "\n",
        "* (<font color='red'>*punna sai teja*</font>)\n",
        "\n",
        "**Include 'shared link' to 'your' final version of this NB here:** (<font color='red'>*enter link from above right here*</font>)\n",
        "* [How to share Colab Notebook link](https://www.tutorialspoint.com/google_colab/google_colab_sharing_notebook.htm)\n",
        "*Expand all notebook bocks/sections and ensure code has properly executed, save as pdf (above left menu) when completed to submit to Canvas.*\n",
        "\n",
        "**Include 'shared link' to 'your' ChatGPT session here:** (<font color='red'>*https://chat.openai.com/share/e4997214-d9c3-4658-b056-e35c3405ed2e*</font>)\n",
        "* [How to share ChatGPT session link](https://www.zdnet.com/article/chatgpt-now-lets-you-create-and-share-links-to-your-conversations/)"
      ],
      "metadata": {
        "id": "bv1zajHXXsMX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPc4CuJc--cX"
      },
      "source": [
        "Useful references and sources:\n",
        "\n",
        "https://www.kaggle.com/c/cifar-10\n",
        "\n",
        "https://keras.io/examples/vision/metric_learning/\n",
        "\n",
        "https://www.kaggle.com/roblexnana/cifar10-with-cnn-for-beginer\n",
        "\n",
        "https://chat.openai.com/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Cite your sources** *(OPTIONAL if none used beyond those above)*\n",
        "\n",
        "Add your own sources and references here."
      ],
      "metadata": {
        "id": "QCrln_CXBtaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview**\n",
        "In this Project we will be exploring ChatGPT and its ability to respond effectively to prompts related to the models provided below."
      ],
      "metadata": {
        "id": "aN8lZu3eYRTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras CIFAR-10 Image Classifier\n"
      ],
      "metadata": {
        "id": "IIszp05trFWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See chat: https://chat.openai.com/share/91efd7f2-0eb5-4814-a146-8e812d6eb3b1"
      ],
      "metadata": {
        "id": "t8B8FPtnsbPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt 1:** Write keras code to build an image classifier for the cifar-10 dataset"
      ],
      "metadata": {
        "id": "3hzikaIxrJmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ChatGPT Solution:**"
      ],
      "metadata": {
        "id": "-LW9fJQYqU-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, utils, datasets\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoded vectors\n",
        "y_train = utils.to_categorical(y_train, 10)\n",
        "y_test = utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the CNN model\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "id": "2Ul-4v3mrU43",
        "outputId": "ffcf99cc-b0e4-4d97-d5f1-5cc95ab713ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 15s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "704/704 [==============================] - 15s 6ms/step - loss: 1.6019 - accuracy: 0.4159 - val_loss: 1.3662 - val_accuracy: 0.5010\n",
            "Epoch 2/10\n",
            "704/704 [==============================] - 4s 6ms/step - loss: 1.2424 - accuracy: 0.5584 - val_loss: 1.1311 - val_accuracy: 0.5970\n",
            "Epoch 3/10\n",
            "704/704 [==============================] - 3s 5ms/step - loss: 1.1039 - accuracy: 0.6106 - val_loss: 1.0859 - val_accuracy: 0.6154\n",
            "Epoch 4/10\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 1.0120 - accuracy: 0.6447 - val_loss: 0.9877 - val_accuracy: 0.6648\n",
            "Epoch 5/10\n",
            "704/704 [==============================] - 4s 6ms/step - loss: 0.9343 - accuracy: 0.6712 - val_loss: 0.9457 - val_accuracy: 0.6754\n",
            "Epoch 6/10\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 0.8745 - accuracy: 0.6941 - val_loss: 0.9049 - val_accuracy: 0.6916\n",
            "Epoch 7/10\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 0.8295 - accuracy: 0.7098 - val_loss: 0.8928 - val_accuracy: 0.6968\n",
            "Epoch 8/10\n",
            "704/704 [==============================] - 4s 6ms/step - loss: 0.7881 - accuracy: 0.7238 - val_loss: 0.8453 - val_accuracy: 0.7204\n",
            "Epoch 9/10\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 0.7430 - accuracy: 0.7407 - val_loss: 0.8703 - val_accuracy: 0.7030\n",
            "Epoch 10/10\n",
            "704/704 [==============================] - 4s 5ms/step - loss: 0.7155 - accuracy: 0.7499 - val_loss: 0.8305 - val_accuracy: 0.7164\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8613 - accuracy: 0.6952\n",
            "Test accuracy: 0.6952000260353088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation:**\n",
        "Did ChatGPT provide a solution that met the expectations of the prompt?\n",
        "\n",
        "The solution provided by ChatGPT was effective in addressing the prompt provided. One of the library imports was deprecated but I am not surprised given the lag between the data that ChatGpt has access to and the libraries Colab is using.\n"
      ],
      "metadata": {
        "id": "2rME52yVraES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt 2:** Explain the code above step by step"
      ],
      "metadata": {
        "id": "S_kyxjsgrkL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ChatGPT Solution:**"
      ],
      "metadata": {
        "id": "8E7rXtp3rzKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Let's go through the code step by step:"
      ],
      "metadata": {
        "id": "MTcdtLgUvVVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, utils, datasets"
      ],
      "metadata": {
        "id": "KJBPnzpEr0sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import NumPy for array operations, TensorFlow for building the neural network, and specific modules from Keras to define the model, load the CIFAR-10 dataset, and perform one-hot encoding for the labels."
      ],
      "metadata": {
        "id": "Eb2e8l5Vr_Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load the CIFAR-10 dataset:\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n"
      ],
      "metadata": {
        "id": "HyeRx2cMr6_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CIFAR-10 dataset is already split into training and test sets. x_train and x_test contain the images, and y_train and y_test contain the corresponding labels."
      ],
      "metadata": {
        "id": "-2z1tTxpeyPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Normalize the pixel values:\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n"
      ],
      "metadata": {
        "id": "xCgbLstAsIxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here, we convert the pixel values of the input images to floating-point numbers and scale them between 0 and 1 by dividing them by 255.0. This normalization step helps in stabilizing the training process."
      ],
      "metadata": {
        "id": "UKJjExZArmjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Convert class vectors to binary class matrices:\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=10)\n"
      ],
      "metadata": {
        "id": "Rb3gqZAxsg4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIFAR-10 has 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). We convert the integer labels to one-hot encoded vectors. For example, if a label is 2, it will be converted to [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], corresponding to the bird class."
      ],
      "metadata": {
        "id": "hN01gq7RspYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Define the model architecture:\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "6htU-yh7teO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a sequential model and add layers one by one. The layers are as follows:\n",
        "\n",
        "First Conv2D layer: 32 filters of size (3, 3) with ReLU activation, taking input shape (32, 32, 3) for RGB images.\n",
        "MaxPooling2D layer: Pooling layer to reduce spatial dimensions by taking the maximum value in a 2x2 window.\n",
        "Second Conv2D layer: 64 filters of size (3, 3) with ReLU activation.\n",
        "Another MaxPooling2D layer.\n",
        "Third Conv2D layer: 64 filters of size (3, 3) with ReLU activation.\n",
        "Flatten layer: Flatten the 3D output from the previous convolutional layers to a 1D vector for the fully connected layers.\n",
        "First Dense layer: 64 neurons with ReLU activation.\n",
        "Second Dense layer: 10 neurons (one for each class) with a softmax activation, giving probabilities for each class."
      ],
      "metadata": {
        "id": "VxaSxFyLsufK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Compile the model:\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZGeRygmlsund"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we configure the model for training by specifying the loss function, optimizer, and evaluation metrics. Since this is a multi-class classification problem, we use \"**categorical_crossentropy**\" as the loss function. We choose the Adam optimizer and monitor the accuracy metric during training."
      ],
      "metadata": {
        "id": "OY6zuGH6tUjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.Train the model:\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "_nZo2cTJsu7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7ae722-8bdb-454d-f72f-01708c8223b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "704/704 [==============================] - 8s 9ms/step - loss: 1.6431 - accuracy: 0.3999 - val_loss: 1.2628 - val_accuracy: 0.5604\n",
            "Epoch 2/10\n",
            "704/704 [==============================] - 6s 8ms/step - loss: 1.3125 - accuracy: 0.5298 - val_loss: 1.0651 - val_accuracy: 0.6360\n",
            "Epoch 3/10\n",
            "704/704 [==============================] - 6s 8ms/step - loss: 1.1802 - accuracy: 0.5787 - val_loss: 1.0035 - val_accuracy: 0.6558\n",
            "Epoch 4/10\n",
            "704/704 [==============================] - 6s 8ms/step - loss: 1.0981 - accuracy: 0.6083 - val_loss: 0.9502 - val_accuracy: 0.6722\n",
            "Epoch 5/10\n",
            "704/704 [==============================] - 6s 8ms/step - loss: 1.0388 - accuracy: 0.6309 - val_loss: 0.9419 - val_accuracy: 0.6678\n",
            "Epoch 6/10\n",
            "704/704 [==============================] - 6s 8ms/step - loss: 0.9892 - accuracy: 0.6477 - val_loss: 0.9000 - val_accuracy: 0.6924\n",
            "Epoch 7/10\n",
            "704/704 [==============================] - 6s 8ms/step - loss: 0.9303 - accuracy: 0.6678 - val_loss: 0.9145 - val_accuracy: 0.6814\n",
            "Epoch 8/10\n",
            "704/704 [==============================] - 6s 8ms/step - loss: 0.8982 - accuracy: 0.6799 - val_loss: 0.8858 - val_accuracy: 0.6906\n",
            "Epoch 9/10\n",
            "704/704 [==============================] - 6s 8ms/step - loss: 0.8541 - accuracy: 0.6926 - val_loss: 0.8698 - val_accuracy: 0.6976\n",
            "Epoch 10/10\n",
            "704/704 [==============================] - 5s 8ms/step - loss: 0.8249 - accuracy: 0.7052 - val_loss: 0.8720 - val_accuracy: 0.7048\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e10b053f310>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This code trains the model on the training data. We use the fit() function, passing in the training data (x_train and y_train), batch size, number of epochs, and a validation split of 0.1 (10% of the training data) for monitoring validation accuracy."
      ],
      "metadata": {
        "id": "5M9st7Uptd2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Evaluate the model:\n",
        "_, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "ljyKe1ett7KA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34bf965c-50db-4a73-c1ad-5849c8674313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.9071 - accuracy: 0.6893\n",
            "Test accuracy: 0.689300000667572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we evaluate the model on the test dataset and print the test accuracy. This will give us an idea of how well the model generalizes to unseen data."
      ],
      "metadata": {
        "id": "_ifqoj0fsuxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation:**\n",
        "Did ChatGPT provide asep by step explanation of the prompt that met expectations?\n",
        "\n",
        "The explanation provided by ChatGPT was clear and concise. All steps were explained logically and effectively.\n"
      ],
      "metadata": {
        "id": "WUnEIEiLvjvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt 3:** Modify the solution to use a pretrained model"
      ],
      "metadata": {
        "id": "Yh7MucSGuOTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ChatGPT Solution:**"
      ],
      "metadata": {
        "id": "KZKW1o4kuVxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Load pre-trained ResNet50 model without the top classification layer\n",
        "base_model = ResNet50(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(32, 32, 3),\n",
        "    pooling=\"avg\",\n",
        ")\n",
        "\n",
        "# Freeze the base model's layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the classifier on top of the base model\n",
        "model = keras.Sequential([\n",
        "    base_model,\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4Ptnn4msvNR",
        "outputId": "b77415fb-6ecb-48c6-f818-facdaa915dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n",
            "Epoch 1/10\n",
            "704/704 [==============================] - 17s 17ms/step - loss: 2.0873 - accuracy: 0.2360 - val_loss: 1.9356 - val_accuracy: 0.3126\n",
            "Epoch 2/10\n",
            "704/704 [==============================] - 10s 14ms/step - loss: 1.9163 - accuracy: 0.3076 - val_loss: 1.8485 - val_accuracy: 0.3186\n",
            "Epoch 3/10\n",
            "704/704 [==============================] - 10s 14ms/step - loss: 1.8511 - accuracy: 0.3306 - val_loss: 1.7991 - val_accuracy: 0.3622\n",
            "Epoch 4/10\n",
            "704/704 [==============================] - 10s 15ms/step - loss: 1.8128 - accuracy: 0.3460 - val_loss: 1.7902 - val_accuracy: 0.3460\n",
            "Epoch 5/10\n",
            "704/704 [==============================] - 10s 15ms/step - loss: 1.7862 - accuracy: 0.3575 - val_loss: 1.7573 - val_accuracy: 0.3640\n",
            "Epoch 6/10\n",
            "704/704 [==============================] - 12s 17ms/step - loss: 1.7583 - accuracy: 0.3706 - val_loss: 1.7278 - val_accuracy: 0.3826\n",
            "Epoch 7/10\n",
            "704/704 [==============================] - 10s 15ms/step - loss: 1.7453 - accuracy: 0.3745 - val_loss: 1.7550 - val_accuracy: 0.3632\n",
            "Epoch 8/10\n",
            "704/704 [==============================] - 10s 14ms/step - loss: 1.7350 - accuracy: 0.3779 - val_loss: 1.7321 - val_accuracy: 0.3740\n",
            "Epoch 9/10\n",
            "704/704 [==============================] - 9s 13ms/step - loss: 1.7217 - accuracy: 0.3831 - val_loss: 1.7086 - val_accuracy: 0.3862\n",
            "Epoch 10/10\n",
            "704/704 [==============================] - 10s 15ms/step - loss: 1.7092 - accuracy: 0.3875 - val_loss: 1.6877 - val_accuracy: 0.3988\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 1.6995 - accuracy: 0.3908\n",
            "Test accuracy: 0.39079999923706055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation:**\n",
        "***Did ChatGPT provide a solution that met the expectations of the prompt?***\n",
        "\n",
        "\n",
        "The solution worked, but the results were far lower than the previous model's results... ~38% accuracy compared to ~70% of the previous version.\n",
        "\n",
        "Aside from the efficiency, ChatGPT did produce a working solution to the problem presented in the prompt.\n"
      ],
      "metadata": {
        "id": "HgbWQmqnxf3A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3eIoUsaOBxXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt 4:** Do it again using vision transformers instead of *convnet*"
      ],
      "metadata": {
        "id": "-DI9c0tdyQYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ChatGPT Solution:**"
      ],
      "metadata": {
        "id": "UW8MklrAyfxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required library not provided in ChatGPT solution but noted in solution notes"
      ],
      "metadata": {
        "id": "JKIh1Xwk7T86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit_keras\n",
        "!pip install tensorflow-addons\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdkCl3XF7PMz",
        "outputId": "7ffbc870-de56-499f-a148-bde7b44a25d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit_keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit_keras) (1.10.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit_keras) (0.20.0)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->vit_keras) (1.22.4)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators->vit_keras) (4.4.2)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of how to use Vision Transformers (ViT) for image classification on the CIFAR-10 dataset:"
      ],
      "metadata": {
        "id": "c2uw6X7NsvDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from vit_keras import vit\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Define the ViT model\n",
        "model = vit.vit_b16(\n",
        "    image_size=32,\n",
        "    activation=\"softmax\",\n",
        "    pretrained=True,\n",
        "    include_top=True,\n",
        "    pretrained_top=True,\n",
        "    classes=10,\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "-76xRhwHsvVU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6176c94-b1fb-4fc6-b2ee-26cd332fafd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/vit_keras/vit.py:139: UserWarning: Can only use pretrained_top with imagenet21k+imagenet2012 if classes = 1000. Setting manually.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n",
            "347502902/347502902 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 2, 2\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7271f0d4cfbf>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 10) and (None, 1000) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation:**\n",
        "***Did ChatGPT provide a solution that met the expectations of the prompt?***\n",
        "\n",
        "I had to install the necessary libraries, including the vit module from the vit_keras library.\n",
        "\n",
        "I initially got an error: **ValueError: Shapes (None, 10) and (None, 1000) are incompatible**\n",
        "\n",
        "I submitted the following prompt ***(I initially got an error: ValueError: Shapes (None, 10) and (None, 1000) are incompatible from the following code please resolve this issue: )*** to ChatGPT and this was the resolution:\n",
        "\n"
      ],
      "metadata": {
        "id": "a5mVQMrEyjLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error you encountered, ValueError: Shapes (None, 10) and (None, 1000) are incompatible, suggests an inconsistency in the number of classes used for one-hot encoding the labels.\n",
        "\n",
        "To resolve this issue, you need to ensure that the number of classes in your CIFAR-10 dataset matches the specified num_classes parameter in the to_categorical function. In the provided code, num_classes is set to 10 for CIFAR-10, which is correct. However, there might be a discrepancy in the loaded dataset.\n",
        "\n",
        "Please try the following steps to resolve the error:\n",
        "\n",
        "Verify that you have imported the correct CIFAR-10 dataset from tensorflow.keras.datasets.cifar10 by double-checking the source and the correctness of the data.\n",
        "\n",
        "Ensure that the labels in your CIFAR-10 dataset have values ranging from 0 to 9. If not, it might indicate an issue with the dataset loading or preprocessing.\n",
        "\n",
        "Check the shape of y_train and y_test arrays to confirm that they have the shape (num_samples, 1) or (num_samples,). If the shape is different, you may need to reshape the labels accordingly.\n",
        "\n",
        "Here's an updated version of the code that includes the reshaping of labels if necessary:\n",
        "\n",
        "**(this solution as well as 7 different ChatGPT generated version did not function.  Then is changed the strategy to:)**\n",
        "The error you encountered is due to incompatible shapes in the output of the ViT model and the expected output shape for the classification task. This issue arises because the ViT model expects an input image size of 224x224, while the CIFAR-10 dataset contains images of size 32x32. To resolve this error, we need to adjust the input image size to match the expected input size of the ViT model.\n",
        "\n",
        "Here's the modified code that resizes the CIFAR-10 images to 224x224:\n",
        "\n",
        "**This solution is --VERY-- RAM intensive**\n",
        "unable to run as it maxes out my capacity in the upgraded Colab subscription withing 4 minutes each time\n",
        "\n",
        " ***\"Your session crashed after using all available RAM. \"***"
      ],
      "metadata": {
        "id": "Too1dG0O9gX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "from tensorflow.keras.preprocessing.image import smart_resize\n",
        "\n",
        "from vit_keras import vit\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Resize images to 224x224\n",
        "x_train_resized = []\n",
        "x_test_resized = []\n",
        "\n",
        "for img in x_train:\n",
        "    img_resized = smart_resize(img, (224, 224))\n",
        "    x_train_resized.append(img_resized)\n",
        "\n",
        "for img in x_test:\n",
        "    img_resized = smart_resize(img, (224, 224))\n",
        "    x_test_resized.append(img_resized)\n",
        "\n",
        "# Normalize pixel values between 0 and 1 and preprocess the input\n",
        "x_train = tf.convert_to_tensor(x_train_resized, dtype=tf.float32) / 255.0\n",
        "x_test = tf.convert_to_tensor(x_test_resized, dtype=tf.float32) / 255.0\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Preprocess input for ViT\n",
        "x_train = preprocess_input(x_train)\n",
        "x_test = preprocess_input(x_test)\n",
        "\n",
        "# Define the ViT model\n",
        "model = vit.vit_b16(\n",
        "    image_size=224,  # Adjusted image size to 224x224\n",
        "    activation=\"softmax\",\n",
        "    pretrained=True,\n",
        "    include_top=True,\n",
        "    pretrained_top=True,\n",
        "    classes=10,\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "Jarb45sm8K6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt 5:** Explain the solution above step by step"
      ],
      "metadata": {
        "id": "wOd-WBsEysmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ChatGPT Solution:**"
      ],
      "metadata": {
        "id": "lStt8s2byykl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go through the code step by step:"
      ],
      "metadata": {
        "id": "eduZ2lJKy-zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import the necessary libraries:\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from vit_keras import vit\n"
      ],
      "metadata": {
        "id": "uEh_KVsWyzgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Load the CIFAR-10 dataset:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
      ],
      "metadata": {
        "id": "3egup9ZRzA9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet loads the CIFAR-10 dataset from Keras and splits it into training and testing sets. The x_train and x_test variables contain the input images, while y_train and y_test contain the corresponding labels."
      ],
      "metadata": {
        "id": "lqJmXg4YzHAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Normalize the pixel values and preprocess the input:\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = preprocess_input(x_train)\n",
        "x_test = preprocess_input(x_test)\n"
      ],
      "metadata": {
        "id": "tOdl67XlzIyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we convert the pixel values of the input images to floating-point numbers and scale them between 0 and 1 by dividing them by 255.0. Additionally, we preprocess the input using the **preprocess_input** function from the VGG19 model. This preprocessing step helps in preparing the images for the Vision Transformer (ViT) model."
      ],
      "metadata": {
        "id": "gvwb60p7zNun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Convert class vectors to binary class matrices:\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=10)\n"
      ],
      "metadata": {
        "id": "rQus9PgvzBHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since CIFAR-10 has 10 classes, we use the to_categorical() function to convert the integer class labels into binary class matrices. This step is essential for multi-class classification."
      ],
      "metadata": {
        "id": "TuogrYyTzWmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Define the ViT model:\n",
        "model = vit.vit_b16(\n",
        "    image_size=32,\n",
        "    activation=\"softmax\",\n",
        "    pretrained=True,\n",
        "    include_top=True,\n",
        "    pretrained_top=True,\n",
        "    classes=10,\n",
        ")\n"
      ],
      "metadata": {
        "id": "c67aRuHIzBMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we define the ViT model using the vit_b16 function from the vit_keras library. We specify the image size as 32x32, set the activation function to \"softmax\", use a pretrained model with weights, include the top classification layer, use pretrained weights for the top classification layer, and set the number of classes to 10."
      ],
      "metadata": {
        "id": "s-7FzCLMzd3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Compile the model:\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "JGFe-YDKzBQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we configure the model for training by specifying the Adam optimizer with a learning rate of 0.001. We use the categorical cross-entropy loss function for multi-class classification and monitor accuracy as the evaluation metric."
      ],
      "metadata": {
        "id": "ykPA5hsEzgz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train the model:\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "XjMVHRmvzg-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains the model on the training data. We use the **fit()** function, passing in the training data (**x_train** and **y_train**), batch size, number of epochs, and a validation split of 0.1 (10% of the training data) for monitoring validation accuracy."
      ],
      "metadata": {
        "id": "xbK4XC4JzhLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Evaluate the model:\n",
        "_, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "nJK4IcZEzhV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, we evaluate the model on the test data using the **evaluate()** function. It returns the loss value and accuracy of the model. Finally, we print the test accuracy.\n",
        "\n",
        "That's it! This code uses Vision Transformers (ViT) for image classification on the CIFAR-10 dataset. It demonstrates how to preprocess the input, define the ViT model, compile it with appropriate settings, train it, and evaluate its performance."
      ],
      "metadata": {
        "id": "NcHIZqFSz7hY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation:**\n",
        "***Did ChatGPT provide asep by step explanation of the prompt that met expectations?***\n",
        "\n",
        "The explanation provided by ChatGPT was clear and concise. All steps were explained logically and effectively. However, the solution did not function due to issues in the code that ChatGPT was unable to resolve to my satisfaction. The last version of a solution provided by ChatGPT may have been functional but required more RAM than I had available.\n",
        "\n",
        "**further exploration required.**\n"
      ],
      "metadata": {
        "id": "bdqPNsRlCINi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt 6:** can you use clip for such a task?"
      ],
      "metadata": {
        "id": "KfNGldXW0Fbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ChatGPT Solution:**"
      ],
      "metadata": {
        "id": "o_5jQmas0Fbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of how to use CLIP for image classification on the CIFAR-10 dataset:"
      ],
      "metadata": {
        "id": "ReME4n670Fb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this code assumes you have installed the necessary libraries, including the torch, torchvision, Pillow, and clip packages. You can install them using pip install torch torchvision pillow ftfy clip."
      ],
      "metadata": {
        "id": "eNFNStUP09Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install torch torchvision ftfy regex pillow clip\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV2RBKz401D9",
        "outputId": "90ab295c-3040-419b-81a5-ad2c62ad04ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-e3n95fnr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-e3n95fnr\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "from PIL import Image\n",
        "import clip\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224),\n",
        "])\n",
        "testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Load the CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Class labels for CIFAR-10\n",
        "class_labels = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "# Perform inference and print predictions\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.item()\n",
        "        text_inputs = [\"a photo of a \" + class_labels[labels]]\n",
        "        text_features = model.encode_text(clip.tokenize(text_inputs).to(device))\n",
        "        image_features = model.encode_image(images)\n",
        "        logits_per_image, _ = model(image=images, text=text_inputs)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "        predicted_label = np.argmax(probs)\n",
        "        if predicted_label == labels:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        print(f\"Predicted: {class_labels[predicted_label]}, Actual: {class_labels[labels]}\")\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "tU9nrEG50Fb0",
        "outputId": "809f1734-1379-42e5-f027-d376bb114707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8aa64bdaad84>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Load the CLIP model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ViT-B/32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Class labels for CIFAR-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation:**\n",
        "Did ChatGPT provide a solution that met the expectations of the prompt?\n",
        "\n",
        "**TypeError: embedding():** argument 'indices' (position 2) must be Tensor, not list\n",
        "\n",
        "I submitted a prompt with the error for ChayGPT to resolve:\n",
        "\n",
        "The error you encountered is due to the clip.tokenize function expecting a tensor as input, but it is receiving a list instead. To resolve this error, you need to convert the text_inputs list into a tensor before passing it to clip.tokenize.\n",
        "\n",
        "Here's the modified code that resolves the error:\n"
      ],
      "metadata": {
        "id": "d11Mggbv0Fb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "import clip\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224),\n",
        "])\n",
        "testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Load the CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Class labels for CIFAR-10\n",
        "class_labels = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "# Perform inference and print predictions\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.item()\n",
        "        text_inputs = [\"a photo of a \" + class_labels[labels]]\n",
        "        text_inputs = torch.tensor(text_inputs).to(device)  # Convert list to tensor\n",
        "        text_features = model.encode_text(clip.tokenize(text_inputs).to(device))\n",
        "        image_features = model.encode_image(images)\n",
        "        logits_per_image, _ = model(image=images, text=text_inputs)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "        predicted_label = np.argmax(probs)\n",
        "        if predicted_label == labels:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        print(f\"Predicted: {class_labels[predicted_label]}, Actual: {class_labels[labels]}\")\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "l4UhLhgXE2GG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "62ac891a-42f8-4448-fe12-86ec18722fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-60dfb2105c1e>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load the CLIP model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-B/32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Class labels for CIFAR-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The error you encountered is due to passing a string (\"a photo of a \") to torch.tensor, which expects a numerical input. To resolve this error, you need to convert the string into a list containing the string before converting it to a tensor.\n",
        "\n",
        "Here's the modified code that resolves the error:"
      ],
      "metadata": {
        "id": "oV7Qb0XsFSTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "import clip\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224),\n",
        "])\n",
        "testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Load the CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Class labels for CIFAR-10\n",
        "class_labels = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "# Perform inference and print predictions\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.item()\n",
        "        text_inputs = [\"a photo of a \" + class_labels[labels]]\n",
        "        text_inputs = [text_inputs]  # Convert string to a list\n",
        "        text_inputs = torch.tensor(text_inputs).to(device)  # Convert list to tensor\n",
        "        text_features = model.encode_text(clip.tokenize(text_inputs).to(device))\n",
        "        image_features = model.encode_image(images)\n",
        "        logits_per_image, _ = model(image=images, text=text_inputs)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "        predicted_label = np.argmax(probs)\n",
        "        if predicted_label == labels:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        print(f\"Predicted: {class_labels[predicted_label]}, Actual: {class_labels[labels]}\")\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "41ORz11LFNUf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "233f23f0-4d7e-4388-822a-a6a63dbe074e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-34d8405abe45>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load the CLIP model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-B/32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Class labels for CIFAR-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT cannot resolve the tensor issue to successfully use clip"
      ],
      "metadata": {
        "id": "xRws_irMFZgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt 7:** Can you rewrite the code above for Keras (instead of PyTorch)?"
      ],
      "metadata": {
        "id": "kj6JNSBP0iBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ChatGPT Solution:**"
      ],
      "metadata": {
        "id": "UqofpYkT0iBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the code rewritten using Keras instead of PyTorch:"
      ],
      "metadata": {
        "id": "60OlEJY80iBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this code assumes you have installed the necessary libraries, including tensorflow, numpy, and clip. You can install them using pip install tensorflow numpy git + https://github.com/openai/CLIP.git."
      ],
      "metadata": {
        "id": "jQAMuG2R1CFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2Nom6Ck1Ife",
        "outputId": "2f23d85d-83c9-4aeb-cf0f-5e5a8f435750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.13)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmE0yGvK57Cb",
        "outputId": "b1019fa9-7320-418d-b6d7-97534f3ecc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: clip in /usr/local/lib/python3.10/dist-packages (1.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224),\n",
        "])\n",
        "testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Load the CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device=device)\n",
        "\n",
        "# Class labels for CIFAR-10\n",
        "class_labels = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "# Perform inference and print predictions\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.item()\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {class_labels[labels]}\")] * images.shape[0]).to(device)\n",
        "        image_features = model.encode_image(images)\n",
        "        text_features = model.encode_text(text_inputs)\n",
        "        logits_per_image, logits_per_text = model(image_features, text_features)\n",
        "        probs = logits_per_image.softmax(dim=1).cpu().numpy()\n",
        "        predicted_label = np.argmax(probs)\n",
        "        if predicted_label == labels:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        print(f\"Predicted: {class_labels[predicted_label]}, Actual: {class_labels[labels]}\")\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "ZmTQ0n630iBP",
        "outputId": "21ad7fd5-331d-45d7-c19d-fabfd329f4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4e1815547f08>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Load the CLIP model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ViT-B/32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Class labels for CIFAR-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation:**\n",
        "Did ChatGPT provide a solution that met the expectations of the prompt?\n",
        "\n",
        "ChatGPT provided a solution, but the solution was not functional. I applied a variety of different strategies to resolve the issue and was unsuccessful in getting clip to work with keras. From what I found it has to do with the clip library. Many recommendations on discussion boards pointed toward a different clip library but none of those were successful resolutions.\n"
      ],
      "metadata": {
        "id": "3KwAHBXV6eJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusions**\n",
        "\n",
        "Based on the interactions with ChatGPT and the Prompts above, I now have the following perceptions of ChatGPT's abilities related to this experience\n",
        "\n",
        "\n",
        "1. **Traditional Machine Learning Models:** The initial experiments with traditional machine learning models like Logistic Regression, Random Forest, and Gradient Boosting showed limited performance on the CIFAR-10 dataset. These models struggled to capture the complex patterns in the images, leading to relatively low accuracy.\n",
        "\n",
        "2. **Deep Learning Models:** The use of deep learning models, particularly Convolutional Neural Networks (CNNs), significantly improved the performance on the CIFAR-10 dataset. The CNN architecture, designed to extract hierarchical features from images, proved to be more effective in image classification tasks.\n",
        "\n",
        "3. **Transfer Learning:** Transfer learning with pre-trained models like VGG19 demonstrated the power of leveraging knowledge from large datasets to improve performance on smaller datasets like CIFAR-10. By fine-tuning the pre-trained model, we achieved higher accuracy compared to training from scratch.\n",
        "\n",
        "4. **Vision Transformer (ViT):** The Vision Transformer, a transformer-based model adapted for image classification, showed competitive performance with traditional CNN architectures. ViT's ability to handle sequences of image patches demonstrated its potential in vision tasks.\n",
        "\n",
        "5. **CLIP Model:** The CLIP model, which combines vision and language understanding, provides a powerful approach for image classification tasks. However, the implementation details and compatibility of the `clip` library might affect the successful execution of the code. It's essential to ensure the correct library versions and method calls when working with CLIP.\n",
        "\n",
        "Conclusions:\n",
        "\n",
        "1. Deep learning models, particularly CNNs, are highly effective in image classification tasks and should be the go-to choice for such problems.\n",
        "\n",
        "2. Transfer learning with pre-trained models saves time and resources, especially when dealing with limited labeled data. Fine-tuning the pre-trained models on the target dataset can lead to improved performance.\n",
        "\n",
        "3. Transformer-based models, like ViT, show promise in computer vision tasks, and their performance is comparable to traditional CNNs.\n",
        "\n",
        "4. Novel approaches like CLIP, which combine vision and language understanding, offer exciting potential for image understanding tasks. However, their implementation might require more careful consideration due to evolving libraries and APIs.\n",
        "\n",
        "5. While deep learning models can achieve impressive results, they are often computationally expensive and require powerful hardware (GPUs or TPUs) for efficient training.\n",
        "\n",
        "If you had more time available, you could explore the following:\n",
        "\n",
        "1. **Hyperparameter Tuning:** Perform an in-depth hyperparameter search to fine-tune the models further.\n",
        "\n",
        "2. **Data Augmentation:** Experiment with different data augmentation techniques to enhance the model's generalization ability.\n",
        "\n",
        "3. **Ensemble Methods:** Combine predictions from multiple models (e.g., CNN, ViT) to create an ensemble and potentially improve overall accuracy.\n",
        "\n",
        "4. **Handling Imbalanced Classes:** The CIFAR-10 dataset is balanced, but in real-world scenarios, dealing with imbalanced classes becomes crucial. Implement techniques like class weighting or data balancing to handle such cases.\n",
        "\n",
        "5. **Interpretability:** Investigate methods for interpreting model predictions to gain insights into what the models are learning.\n",
        "\n",
        "6. **Exploring CLIP in Detail:** If you are particularly interested in the CLIP model, take time to explore the official documentation and community forums to ensure correct usage and compatibility with the library.\n",
        "\n",
        "Overall, image classification is an exciting field with continuous advancements. Keep experimenting and learning to stay up-to-date with the latest techniques and improve your models further."
      ],
      "metadata": {
        "id": "ENTr2J49Ft0P"
      }
    }
  ]
}