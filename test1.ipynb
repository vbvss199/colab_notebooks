{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaMQ0o32MoCff5wvSY/8Sr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbvss199/colab_notebooks/blob/main/test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNPk_e55W7IV"
      },
      "outputs": [],
      "source": [
        "#importing all the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.utils.data import DataLoader,TensorDataset, random_split, WeightedRandomSampler\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_excel('fall_processed.xlsx')\n",
        "print(data.columns)\n",
        "print(data.shape)\n",
        "application_id=data['Application Reference ID'].to_numpy()\n",
        "\n",
        "inputs=data.drop(['admitted','Application Reference ID'],axis=1).to_numpy()\n",
        "\n",
        "labels=data['admitted'].to_numpy()\n"
      ],
      "metadata": {
        "id": "cPZgk1-RW-_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    inputs, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "print(inputs.shape[1])\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_inputs = torch.from_numpy(train_inputs).float()\n",
        "test_inputs = torch.from_numpy(test_inputs).float()\n",
        "train_labels = torch.from_numpy(train_labels).int()\n",
        "test_labels = torch.from_numpy(test_labels).int()\n",
        "\n",
        "input_embedding_dimension = int(inputs[:, 0].max()) + 1  # 112\n",
        "input_hs_embedding_dimension = int(inputs[:, 4].max()) + 1\n",
        "\n",
        "print(input_embedding_dimension, input_hs_embedding_dimension)\n",
        "print(\"Max index in categorical column 0:\", inputs[:, 0].max(), \"Embedding dim:\", input_embedding_dimension)\n",
        "print(\"Max index in categorical column 4:\", inputs[:, 4].max(), \"Embedding dim:\", input_hs_embedding_dimension)\n",
        "print(\"Max index in train categorical column 4:\", train_inputs[:, 4].max())\n",
        "print(\"Max index in test categorical column 4:\", test_inputs[:, 4].max())\n",
        "\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "print(\"Checking train_inputs for NaN:\", torch.any(torch.isnan(train_inputs)))\n",
        "print(\"Checking train_inputs for Inf:\", torch.any(torch.isinf(train_inputs)))\n",
        "print(\"Checking train_labels for NaN:\", torch.any(torch.isnan(train_labels)))\n",
        "print(\"Checking train_labels for Inf:\", torch.any(torch.isinf(train_labels)))"
      ],
      "metadata": {
        "id": "O53qjh4jXCGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, i, h_size, h_next_size, h_next_next_size=16, n_classes=2,\n",
        "                 how_many_layers=4, embedding_dim=16,hs_embedding_dim=50):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        features = i.shape[1]  # Total number of input features\n",
        "\n",
        "        self.major_embedding = nn.Embedding(input_embedding_dimension, embedding_dim)\n",
        "\n",
        "        #embedding layer for the high school codes\n",
        "        self.hs_embedding=nn.Embedding(input_hs_embedding_dimension,hs_embedding_dim)\n",
        "\n",
        "        print(f\"Setting major embedding dim: {input_embedding_dimension}\")\n",
        "        print(f\"Setting HS embedding dim: {input_hs_embedding_dimension}\")\n",
        "\n",
        "        # Input to fc1 will be (features - 2) continuous + embedding_dim\n",
        "        self.fc1 = nn.Linear(features - 2 + embedding_dim + hs_embedding_dim, h_size)\n",
        "        self.layers = how_many_layers\n",
        "\n",
        "        if self.layers == 2:\n",
        "            self.fc2 = nn.Linear(h_size, n_classes)\n",
        "        elif self.layers == 3:\n",
        "            self.fc3 = nn.Linear(h_size, h_next_size)\n",
        "            self.fc4 = nn.Linear(h_next_size, n_classes)\n",
        "        elif self.layers == 4:\n",
        "            self.fc3 = nn.Linear(h_size, h_next_size)\n",
        "            self.fc4 = nn.Linear(h_next_size, h_next_next_size)\n",
        "            self.fc5 = nn.Linear(h_next_next_size, n_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Ensure input is float and extract categorical/continuous features\n",
        "        categorical_input = X[:, 0].long()  # First column: categorical (encoded)\n",
        "        hs_input = X[:,4].long()\n",
        "\n",
        "        #continuous_input = X[:, 1:].float() # Rest: continuous features\n",
        "        continuous_indices = [i for i in range(X.shape[1]) if i not in [0, 4]]\n",
        "        continuous_input = X[:, continuous_indices].float()\n",
        "\n",
        "        # Apply embedding\n",
        "        embedded = self.major_embedding(categorical_input)  # Shape: [batch_size, embedding_dim]\n",
        "        hs_embedded = self.hs_embedding(hs_input)\n",
        "\n",
        "        # Concatenate with continuous features\n",
        "        X = torch.cat((embedded,hs_embedded, continuous_input), dim=1)  # Shape: [batch_size, embedding_dim + (features-1)]\n",
        "\n",
        "        if self.layers == 2:\n",
        "            X = F.relu(self.fc1(X))\n",
        "            X = self.fc2(X)\n",
        "        elif self.layers == 3:\n",
        "            X = F.relu(self.fc1(X))\n",
        "            X = F.relu(self.fc3(X))\n",
        "            X = self.fc4(X)\n",
        "        elif self.layers == 4:\n",
        "            X = F.relu(self.fc1(X))\n",
        "            X = torch.tanh(self.fc3(X))\n",
        "            X = F.sigmoid(self.fc4(X))\n",
        "            X = self.fc5(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "net = NeuralNetwork(inputs, h_size=32, h_next_size=48, how_many_layers=3)\n",
        "print(f\"Expected input features to fc1: {train_inputs.shape[1] - 1 + 8}\")  # Debug print\n",
        "print(f\"fc1 weight shape: {net.fc1.weight.shape}\")  # Debug printn_epochs = 600\n",
        "\n",
        "\n",
        "n_epochs = 600\n",
        "learning_rate = 0.0001\n",
        "decay_rate = learning_rate / n_epochs\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=decay_rate)\n",
        "lambda_reg = 0.01\n",
        "lambda_entropy = 0\n",
        "\n",
        "\n",
        "def loss_fn(model, outputs, targets):\n",
        "    # Convert labels to numpy\n",
        "    y_train = train_labels.numpy()\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "    cross_entropy = nn.functional.cross_entropy(outputs, targets,weight=class_weights_tensor)\n",
        "    l2_regularization = 0\n",
        "    entropy_regularization = 0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        l2_regularization += torch.norm(param, p=2) ** 2\n",
        "        entropy_regularization += torch.mean(torch.sum(-outputs * torch.log(outputs), dim=1))\n",
        "\n",
        "    loss = cross_entropy + lambda_reg * l2_regularization\n",
        "    return loss\n",
        "\n",
        "def test_instance(model):\n",
        "    y_t = []\n",
        "    y_s = []\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss += loss_fn(model, outputs, labels.long())\n",
        "            y_t.extend(labels.numpy().astype('int'))\n",
        "            y_s.extend(torch.sigmoid(outputs).max(axis=1).indices.numpy())\n",
        "\n",
        "    acc = accuracy_score(y_t, y_s)\n",
        "    return loss, acc\n",
        "\n",
        "iteration = 0\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    total = 0  # No. of total predictions\n",
        "    correct = 0  # No. of correct predictions\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = loss_fn(net, outputs, labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)  # Loss in every epoch\n",
        "    epoch_acc = correct / total  # Accuracy for every epoch\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
        "        print(f'Epoch: {epoch + 1}/{n_epochs} | pLoss: {running_loss / len(inputs)} | Accuracy: {epoch_acc} | Loss: {epoch_loss}')\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        test_loss, test_acc = test_instance(net)\n",
        "        print(f'Epoch: {epoch + 1} | The test data Accuracy = {test_acc} | Test Loss = {test_loss}')\n",
        "\n",
        "        if counter < test_acc:\n",
        "            save_net = net\n",
        "            counter = test_acc\n",
        "\n",
        "y_true = []\n",
        "y_scores = []\n",
        "test_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = save_net(inputs)\n",
        "        test_loss += loss_fn(net, outputs, labels.long())\n",
        "        y_true.extend(labels.numpy().astype('int'))\n",
        "        y_scores.extend(torch.sigmoid(outputs).max(axis=1).indices.numpy())\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_scores)\n",
        "precision = precision_score(y_true, y_scores)\n",
        "recall = recall_score(y_true, y_scores)\n",
        "f1_val = f1_score(y_true, y_scores)\n",
        "auc_roc = roc_auc_score(y_true, y_scores)\n",
        "\n",
        "print('Accuracy: {:.4f}'.format(accuracy))\n",
        "print('Precision: {:.4f}'.format(precision))\n",
        "print('Recall: {:.4f}'.format(recall))\n",
        "print('F1 Score: {:.4f}'.format(f1_val))\n",
        "print('AUROC Score: {:.4f}'.format(auc_roc))\n"
      ],
      "metadata": {
        "id": "o43x_BODXDsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}